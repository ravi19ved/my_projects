{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "257be074",
   "metadata": {},
   "source": [
    "I'll provide a Python script to convert radiosonde data files to NetCDF format. Since I haven't seen your specific file format, I'll make some assumptions based on common radiosonde data structures. You may need to adjust the script to match your exact file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64460e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def parse_radiosonde_file(file_path):\n",
    "    \"\"\"Parse radiosonde data file with metadata, headers, and data sections\"\"\"\n",
    "    \n",
    "    # Initialize variables\n",
    "    metadata = {}\n",
    "    header = []\n",
    "    data = []\n",
    "    in_header = False\n",
    "    in_data = False\n",
    "    in_metadata = True\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            \n",
    "            # Skip empty lines\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            # Check for metadata (typically key-value pairs)\n",
    "            if in_metadata and ':' in line:\n",
    "                key, value = line.split(':', 1)\n",
    "                metadata[key.strip()] = value.strip()\n",
    "                continue\n",
    "            else:\n",
    "                in_metadata = False\n",
    "                \n",
    "            # Check for header start\n",
    "            if line.startswith('---') or any(word in line.lower() for word in ['press', 'hght', 'temp', 'dwpt']):\n",
    "                in_header = True\n",
    "                header = re.split(r'\\s{2,}', line.strip())  # Split on multiple spaces\n",
    "                continue\n",
    "                \n",
    "            # After header, data begins\n",
    "            if in_header and not in_data:\n",
    "                in_data = True\n",
    "                continue\n",
    "                \n",
    "            # Parse data lines\n",
    "            if in_data:\n",
    "                # Skip section dividers\n",
    "                if line.startswith('---') or line.startswith('Standard') or line.startswith('Significant'):\n",
    "                    continue\n",
    "                # Split data line (assuming space-separated values)\n",
    "                data.append(re.split(r'\\s+', line.strip()))\n",
    "    \n",
    "    # Convert data to DataFrame\n",
    "    df = pd.DataFrame(data, columns=header)\n",
    "    \n",
    "    # Convert numeric columns\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            df[col] = pd.to_numeric(df[col])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return metadata, df\n",
    "\n",
    "def create_netcdf(metadata, df, output_path):\n",
    "    \"\"\"Create NetCDF file from parsed data\"\"\"\n",
    "    \n",
    "    # Extract common metadata\n",
    "    launch_time = metadata.get('Launch Time', '')\n",
    "    station_id = metadata.get('Station Number', '')\n",
    "    station_name = metadata.get('Station Name', '')\n",
    "    \n",
    "    # Convert time string to datetime object\n",
    "    try:\n",
    "        launch_dt = datetime.strptime(launch_time, '%Y-%m-%d %H:%M:%S')\n",
    "    except:\n",
    "        launch_dt = datetime.now()  # fallback\n",
    "    \n",
    "    # Create xarray Dataset\n",
    "    ds = xr.Dataset()\n",
    "    \n",
    "    # Add metadata as global attributes\n",
    "    ds.attrs['title'] = f\"Radiosonde Data - {station_name}\"\n",
    "    ds.attrs['station_id'] = station_id\n",
    "    ds.attrs['station_name'] = station_name\n",
    "    ds.attrs['launch_time'] = launch_time\n",
    "    ds.attrs['source'] = 'Radiosonde'\n",
    "    ds.attrs['history'] = f\"Created {datetime.now().isoformat()}\"\n",
    "    \n",
    "    # Add variables from DataFrame\n",
    "    for col in df.columns:\n",
    "        # Skip non-numeric columns\n",
    "        if not np.issubdtype(df[col].dtype, np.number):\n",
    "            continue\n",
    "            \n",
    "        # Create dimension (assuming height/pressure as primary dimension)\n",
    "        if 'hght' in col.lower() or 'height' in col.lower():\n",
    "            ds['height'] = (('height'), df[col].values)\n",
    "            ds['height'].attrs['units'] = 'm'\n",
    "            ds['height'].attrs['long_name'] = 'Height above sea level'\n",
    "            primary_dim = 'height'\n",
    "        elif 'press' in col.lower() or 'pressure' in col.lower():\n",
    "            ds['pressure'] = (('pressure'), df[col].values)\n",
    "            ds['pressure'].attrs['units'] = 'hPa'\n",
    "            ds['pressure'].attrs['long_name'] = 'Atmospheric pressure'\n",
    "            primary_dim = 'pressure'\n",
    "            \n",
    "    # If no explicit dimension found, use index\n",
    "    if 'primary_dim' not in locals():\n",
    "        ds['index'] = (('index'), np.arange(len(df)))\n",
    "        primary_dim = 'index'\n",
    "    \n",
    "    # Add other variables\n",
    "    for col in df.columns:\n",
    "        if not np.issubdtype(df[col].dtype, np.number):\n",
    "            continue\n",
    "        if col.lower() in ['hght', 'height', 'press', 'pressure']:\n",
    "            continue\n",
    "            \n",
    "        var_name = col.lower().replace(' ', '_')\n",
    "        ds[var_name] = ((primary_dim), df[col].values)\n",
    "        \n",
    "        # Add units if known\n",
    "        if 'temp' in var_name:\n",
    "            ds[var_name].attrs['units'] = 'Celsius'\n",
    "            ds[var_name].attrs['long_name'] = 'Temperature'\n",
    "        elif 'dwpt' in var_name or 'dew' in var_name:\n",
    "            ds[var_name].attrs['units'] = 'Celsius'\n",
    "            ds[var_name].attrs['long_name'] = 'Dew point temperature'\n",
    "        elif 'rh' in var_name:\n",
    "            ds[var_name].attrs['units'] = '%'\n",
    "            ds[var_name].attrs['long_name'] = 'Relative humidity'\n",
    "        elif 'wind' in var_name:\n",
    "            ds[var_name].attrs['units'] = 'm/s'\n",
    "            ds[var_name].attrs['long_name'] = 'Wind speed'\n",
    "    \n",
    "    # Add time coordinate\n",
    "    ds['time'] = launch_dt\n",
    "    ds['time'].attrs['long_name'] = 'Launch time'\n",
    "    \n",
    "    # Save to NetCDF\n",
    "    ds.to_netcdf(output_path)\n",
    "    return ds\n",
    "\n",
    "def process_directory(input_dir, output_dir):\n",
    "    \"\"\"Process all files in a directory\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # List to store all datasets for merging\n",
    "    all_datasets = []\n",
    "    \n",
    "    for filename in os.listdir(input_dir):\n",
    "        if not filename.endswith('.txt'):  # adjust extension as needed\n",
    "            continue\n",
    "            \n",
    "        input_path = os.path.join(input_dir, filename)\n",
    "        output_path = os.path.join(output_dir, f\"{os.path.splitext(filename)[0]}.nc\")\n",
    "        \n",
    "        print(f\"Processing {filename}...\")\n",
    "        try:\n",
    "            metadata, df = parse_radiosonde_file(input_path)\n",
    "            ds = create_netcdf(metadata, df, output_path)\n",
    "            all_datasets.append(ds)\n",
    "            print(f\"Saved to {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {str(e)}\")\n",
    "    \n",
    "    # Combine all files into one NetCDF\n",
    "    if all_datasets:\n",
    "        combined_path = os.path.join(output_dir, \"combined_radiosonde_data.nc\")\n",
    "        combined_ds = xr.concat(all_datasets, dim='time')\n",
    "        combined_ds.to_netcdf(combined_path)\n",
    "        print(f\"Combined all files into {combined_path}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # input_directory = \"path/to/your/radiosonde/files\"\n",
    "    # output_directory = \"path/to/output/netcdf/files\"\n",
    "    input_directory = \".\"\n",
    "    output_directory = \".\"\n",
    "\n",
    "    process_directory(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1358224a",
   "metadata": {},
   "source": [
    "How to Use This Script\n",
    "Save the script as radiosonde_to_netcdf.py\n",
    "\n",
    "Modify the input_directory and output_directory paths at the bottom of the script\n",
    "\n",
    "Run the script: python radiosonde_to_netcdf.py\n",
    "\n",
    "Customizing the Script\n",
    "Since I haven't seen your exact file format, you may need to adjust:\n",
    "\n",
    "Metadata parsing: Modify the parse_radiosonde_file function to match how your metadata is structured\n",
    "\n",
    "Column names: Update the variable detection logic in create_netcdf to match your column headers\n",
    "\n",
    "Data sections: Adjust how the script identifies the start/end of data sections if your format differs\n",
    "\n",
    "Combining Multiple Files\n",
    "The script automatically combines all processed files into a single NetCDF file named combined_radiosonde_data.nc in the output directory. The combination is done along the time dimension.\n",
    "\n",
    "Alternative Approach for Large Datasets\n",
    "If you have hundreds of files and memory becomes an issue, you might want to process them in batches or use Dask for out-of-core computation. Here's a modified version of the combination code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6af6690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import glob\n",
    "\n",
    "def combine_netcdf_files(output_dir):\n",
    "    \"\"\"Combine all NetCDF files in a directory\"\"\"\n",
    "    files = glob.glob(os.path.join(output_dir, \"*.nc\"))\n",
    "    \n",
    "    # Exclude the combined file if it exists\n",
    "    files = [f for f in files if not f.endswith(\"combined_radiosonde_data.nc\")]\n",
    "    \n",
    "    if not files:\n",
    "        print(\"No NetCDF files found to combine\")\n",
    "        return\n",
    "    \n",
    "    # Open all files as a single dataset\n",
    "    combined_ds = xr.open_mfdataset(files, combine='by_coords')\n",
    "    \n",
    "    # Save combined file\n",
    "    combined_path = os.path.join(output_dir, \"combined_radiosonde_data.nc\")\n",
    "    combined_ds.to_netcdf(combined_path)\n",
    "    print(f\"Combined {len(files)} files into {combined_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2689a7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "import os\n",
    "\n",
    "def convert_radiosonde_to_netcdf(file_path, output_dir=\".\"):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        all_lines = f.readlines()\n",
    "\n",
    "    # Metadata\n",
    "    station = all_lines[0].split(':')[1].strip()\n",
    "\n",
    "    # Parse location\n",
    "    location_parts = all_lines[1].split(':')[1].strip().split()\n",
    "    lat = float(location_parts[0])\n",
    "    lon = float(location_parts[1])\n",
    "    alt = float(location_parts[2])  # altitude in meters\n",
    "\n",
    "    # Parse launch time\n",
    "    launch_time_str = all_lines[2].split(':')[1].strip()\n",
    "    launch_time = datetime.strptime(launch_time_str, \"%Y %m %d at %H %M UTC\")\n",
    "\n",
    "    # Radiosonde info\n",
    "    rs_type = all_lines[3].split(':')[1].strip()\n",
    "    rs_number = all_lines[4].split(':')[1].strip()\n",
    "\n",
    "    # Locate start of data block\n",
    "    data_start_idx = next(i for i, line in enumerate(all_lines) if line.strip().startswith('0'))\n",
    "    data_lines = all_lines[data_start_idx:]\n",
    "\n",
    "    # Define columns based on structure\n",
    "    columns = ['min', 'sec', 'Hght_gpm', 'Press_hPa', 'Temp_degC', 'RH_percent',\n",
    "               'DewPoint_degC', 'MixingRatio_gpkg', 'WindDir_deg', 'WindSpd_ms',\n",
    "               'Latitude_deg', 'Longitude_deg']\n",
    "\n",
    "    # Read the data\n",
    "    df = pd.read_csv(StringIO(''.join(data_lines)),\n",
    "                     delim_whitespace=True,\n",
    "                     names=columns,\n",
    "                     engine='python')\n",
    "    \n",
    "    # Drop rows with missing Pressure (filtering interpolated/special levels)\n",
    "    df = df.dropna(subset=['Press_hPa'])\n",
    "\n",
    "    # Compute absolute time\n",
    "    df['TimeSec'] = df['min'] * 60 + df['sec']\n",
    "    time = pd.to_timedelta(df['TimeSec'], unit='s') + pd.Timestamp(launch_time)\n",
    "\n",
    "    # Create xarray Dataset\n",
    "    ds = xr.Dataset(\n",
    "        {\n",
    "            \"Pressure\": (\"time\", df[\"Press_hPa\"]),\n",
    "            \"Temperature\": (\"time\", df[\"Temp_degC\"]),\n",
    "            \"Humidity\": (\"time\", df[\"RH_percent\"]),\n",
    "            \"DewPoint\": (\"time\", df[\"DewPoint_degC\"]),\n",
    "            \"MixingRatio\": (\"time\", df[\"MixingRatio_gpkg\"]),\n",
    "            \"WindDirection\": (\"time\", df[\"WindDir_deg\"]),\n",
    "            \"WindSpeed\": (\"time\", df[\"WindSpd_ms\"]),\n",
    "            \"Height\": (\"time\", df[\"Hght_gpm\"]),\n",
    "            \"Latitude\": (\"time\", df[\"Latitude_deg\"]),\n",
    "            \"Longitude\": (\"time\", df[\"Longitude_deg\"])\n",
    "        },\n",
    "        coords={\n",
    "            \"time\": time\n",
    "        },\n",
    "        attrs={\n",
    "            \"station\": station,\n",
    "            \"latitude\": lat,\n",
    "            \"longitude\": lon,\n",
    "            \"altitude_m\": alt,\n",
    "            \"launch_time\": str(launch_time),\n",
    "            \"RS_type\": rs_type,\n",
    "            \"RS_number\": rs_number\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Output NetCDF file\n",
    "    filename = os.path.splitext(os.path.basename(file_path))[0] + \".nc\"\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "    ds.to_netcdf(output_path)\n",
    "    print(f\"NetCDF saved to: {output_path}\")\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14075e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravic\\AppData\\Local\\Temp\\ipykernel_41900\\3676282465.py:38: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = pd.read_csv(StringIO(''.join(data_lines)),\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'd:\\\\Projects\\\\my_projects\\\\work\\\\radio_sonde\\\\output\\\\summary_2407130557.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ravic\\anaconda3\\envs\\geo_env\\Lib\\site-packages\\xarray\\backends\\file_manager.py:211\u001b[39m, in \u001b[36mCachingFileManager._acquire_with_cache_info\u001b[39m\u001b[34m(self, needs_lock)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m     file = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ravic\\anaconda3\\envs\\geo_env\\Lib\\site-packages\\xarray\\backends\\lru_cache.py:56\u001b[39m, in \u001b[36mLRUCache.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     57\u001b[39m     \u001b[38;5;28mself\u001b[39m._cache.move_to_end(key)\n",
      "\u001b[31mKeyError\u001b[39m: [<class 'netCDF4._netCDF4.Dataset'>, ('d:\\\\Projects\\\\my_projects\\\\work\\\\radio_sonde\\\\output\\\\summary_2407130557.nc',), 'a', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False)), '65a7c1b0-e44c-47e3-901a-957817f49c2d']",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mPermissionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Single file conversion\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mconvert_radiosonde_to_netcdf\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./summary_2407130557.EDT\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./output\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 81\u001b[39m, in \u001b[36mconvert_radiosonde_to_netcdf\u001b[39m\u001b[34m(file_path, output_dir)\u001b[39m\n\u001b[32m     79\u001b[39m filename = os.path.splitext(os.path.basename(file_path))[\u001b[32m0\u001b[39m] + \u001b[33m\"\u001b[39m\u001b[33m.nc\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     80\u001b[39m output_path = os.path.join(output_dir, filename)\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m \u001b[43mds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_netcdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNetCDF saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output_path\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ravic\\anaconda3\\envs\\geo_env\\Lib\\site-packages\\xarray\\core\\dataset.py:2021\u001b[39m, in \u001b[36mDataset.to_netcdf\u001b[39m\u001b[34m(self, path, mode, format, group, engine, encoding, unlimited_dims, compute, invalid_netcdf, auto_complex)\u001b[39m\n\u001b[32m   2018\u001b[39m     encoding = {}\n\u001b[32m   2019\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxarray\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_netcdf\n\u001b[32m-> \u001b[39m\u001b[32m2021\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_netcdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[return-value]  # mypy cannot resolve the overloads:(\u001b[39;49;00m\n\u001b[32m   2022\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2023\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2024\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2025\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2026\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2027\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2028\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2029\u001b[39m \u001b[43m    \u001b[49m\u001b[43munlimited_dims\u001b[49m\u001b[43m=\u001b[49m\u001b[43munlimited_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2030\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2031\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmultifile\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2032\u001b[39m \u001b[43m    \u001b[49m\u001b[43minvalid_netcdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43minvalid_netcdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2033\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauto_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauto_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2034\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ravic\\anaconda3\\envs\\geo_env\\Lib\\site-packages\\xarray\\backends\\api.py:1911\u001b[39m, in \u001b[36mto_netcdf\u001b[39m\u001b[34m(dataset, path_or_file, mode, format, group, engine, encoding, unlimited_dims, compute, multifile, invalid_netcdf, auto_complex)\u001b[39m\n\u001b[32m   1908\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m auto_complex \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1909\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mauto_complex\u001b[39m\u001b[33m\"\u001b[39m] = auto_complex\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m store = \u001b[43mstore_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m unlimited_dims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1914\u001b[39m     unlimited_dims = dataset.encoding.get(\u001b[33m\"\u001b[39m\u001b[33munlimited_dims\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ravic\\anaconda3\\envs\\geo_env\\Lib\\site-packages\\xarray\\backends\\netCDF4_.py:452\u001b[39m, in \u001b[36mNetCDF4DataStore.open\u001b[39m\u001b[34m(cls, filename, mode, format, group, clobber, diskless, persist, auto_complex, lock, lock_maker, autoclose)\u001b[39m\n\u001b[32m    448\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mauto_complex\u001b[39m\u001b[33m\"\u001b[39m] = auto_complex\n\u001b[32m    449\u001b[39m manager = CachingFileManager(\n\u001b[32m    450\u001b[39m     netCDF4.Dataset, filename, mode=mode, kwargs=kwargs\n\u001b[32m    451\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautoclose\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ravic\\anaconda3\\envs\\geo_env\\Lib\\site-packages\\xarray\\backends\\netCDF4_.py:393\u001b[39m, in \u001b[36mNetCDF4DataStore.__init__\u001b[39m\u001b[34m(self, manager, group, mode, lock, autoclose)\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;28mself\u001b[39m._group = group\n\u001b[32m    392\u001b[39m \u001b[38;5;28mself\u001b[39m._mode = mode\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28mself\u001b[39m.format = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mds\u001b[49m.data_model\n\u001b[32m    394\u001b[39m \u001b[38;5;28mself\u001b[39m._filename = \u001b[38;5;28mself\u001b[39m.ds.filepath()\n\u001b[32m    395\u001b[39m \u001b[38;5;28mself\u001b[39m.is_remote = is_remote_uri(\u001b[38;5;28mself\u001b[39m._filename)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ravic\\anaconda3\\envs\\geo_env\\Lib\\site-packages\\xarray\\backends\\netCDF4_.py:461\u001b[39m, in \u001b[36mNetCDF4DataStore.ds\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    459\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    460\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mds\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_acquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ravic\\anaconda3\\envs\\geo_env\\Lib\\site-packages\\xarray\\backends\\netCDF4_.py:455\u001b[39m, in \u001b[36mNetCDF4DataStore._acquire\u001b[39m\u001b[34m(self, needs_lock)\u001b[39m\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_acquire\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneeds_lock\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[43mds\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m_nc4_require_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ravic\\anaconda3\\envs\\geo_env\\Lib\\contextlib.py:141\u001b[39m, in \u001b[36m_GeneratorContextManager.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.kwds, \u001b[38;5;28mself\u001b[39m.func\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgenerator didn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt yield\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ravic\\anaconda3\\envs\\geo_env\\Lib\\site-packages\\xarray\\backends\\file_manager.py:199\u001b[39m, in \u001b[36mCachingFileManager.acquire_context\u001b[39m\u001b[34m(self, needs_lock)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;129m@contextlib\u001b[39m.contextmanager\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34macquire_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    198\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Context manager for acquiring a file.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     file, cached = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_acquire_with_cache_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneeds_lock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    201\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ravic\\anaconda3\\envs\\geo_env\\Lib\\site-packages\\xarray\\backends\\file_manager.py:217\u001b[39m, in \u001b[36mCachingFileManager._acquire_with_cache_info\u001b[39m\u001b[34m(self, needs_lock)\u001b[39m\n\u001b[32m    215\u001b[39m     kwargs = kwargs.copy()\n\u001b[32m    216\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mmode\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m._mode\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m file = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_opener\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._mode == \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# ensure file doesn't get overridden when opened again\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;28mself\u001b[39m._mode = \u001b[33m\"\u001b[39m\u001b[33ma\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc\\\\netCDF4\\\\_netCDF4.pyx:2521\u001b[39m, in \u001b[36mnetCDF4._netCDF4.Dataset.__init__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc\\\\netCDF4\\\\_netCDF4.pyx:2158\u001b[39m, in \u001b[36mnetCDF4._netCDF4._ensure_nc_success\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mPermissionError\u001b[39m: [Errno 13] Permission denied: 'd:\\\\Projects\\\\my_projects\\\\work\\\\radio_sonde\\\\output\\\\summary_2407130557.nc'"
     ]
    }
   ],
   "source": [
    "# Single file conversion\n",
    "convert_radiosonde_to_netcdf(\"summary_2407130557.EDT\", output_dir=\"./output\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
